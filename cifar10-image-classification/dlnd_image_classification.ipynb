{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e91bef208>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x / np.max(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = [None]\n",
    "    for dim in image_shape:\n",
    "        shape.append(dim)\n",
    "    return tf.placeholder(tf.float32, shape, name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = [None, n_classes]\n",
    "    return tf.placeholder(tf.int32, shape, name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # Conv2d parameters\n",
    "    x_depth = x_tensor.get_shape()[-1].value\n",
    "    filter_shape = conv_ksize + (x_depth,) + (conv_num_outputs,)\n",
    "    strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    \n",
    "    F_W = tf.Variable(tf.truncated_normal(filter_shape, mean=0.0, stddev=0.05))\n",
    "    F_b = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    padding = 'SAME'\n",
    "    \n",
    "    # MaxPool parameters\n",
    "    ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    pool_strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    \n",
    "    conv2d = tf.nn.conv2d(x_tensor, F_W, strides, padding) + F_b\n",
    "    conv2d = tf.nn.relu(conv2d)\n",
    "    conv2d = tf.nn.max_pool(conv2d, ksize, pool_strides, padding)\n",
    "    \n",
    "    return conv2d\n",
    "\n",
    "\"\"\"\n",
    "Activations Trial and Error:\n",
    "-> tried tanh(), softplus(), softsign()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    x_shape = x_tensor.get_shape()\n",
    "    flattened_image_size = 1\n",
    "    \n",
    "    for i in range(3):\n",
    "        flattened_image_size *= x_shape[i+1].value\n",
    "    \n",
    "    x_flattened = tf.reshape(x_tensor, [-1, flattened_image_size])\n",
    "    \n",
    "    return x_flattened\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Learnings through Trial & Error with Activations\n",
    "    1. ReluS in both the places don't work: Validation Accuracy drops drastically and stagnates\n",
    "    \n",
    "    2. Activation: using softplus() / softsign() instead of sigmoid() dropped the accuracy from 0.4 to 0.1\n",
    "    \"\"\"\n",
    "    \n",
    "    x_rows = x_tensor.get_shape()[1].value\n",
    "    W = tf.Variable(tf.truncated_normal((x_rows, num_outputs)))\n",
    "    b = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    output = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    # try different activations:\n",
    "    output = tf.nn.sigmoid(output)\n",
    "    output = tf.nn.\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    x_rows = x_tensor.get_shape()[1].value\n",
    "    W = tf.Variable(tf.truncated_normal((x_rows, num_outputs)))\n",
    "    b = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    output = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    return output\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \n",
    "    Learnings through Trial & Error:\n",
    "    1. Setting the parameters to exponents of 2 increases the efficiency of the CNN, whereas setting odd figures \n",
    "        (previous try odd parameters: conv1&2_num_outputs=9&25, conv1&2_ksize=5&3, fully_connected1&2=59&25)\n",
    "    \n",
    "    2. Tried this architecture: conv1->flatten1->fully1->conv2->flatten2->fully2->output \n",
    "        (doesn't work because of rank / shape error somewhere. Checkout later)\n",
    "        \n",
    "    3. Accuracy dropped from 0.4 to 0.2 after introducing conv3 with 64 num_outputs and similar other params as conv2\n",
    "    \n",
    "    4. Accuracy remained unchanged from 0.4 after introducing fully_connected3\n",
    "    \n",
    "    5. \n",
    "    \n",
    "    6. \n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x,       16,               (4, 4),        (1, 1),       (2, 2),     (2, 2))\n",
    "    conv2 = conv2d_maxpool(conv1,   32,               (2, 2),        (2, 2),       (2, 2),     (2, 2))\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flattened_conv = flatten(conv2)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "                #   fully_conn(x_tensor, num_outputs)\n",
    "    fully_connected1 = fully_conn(flattened_conv, 32)\n",
    "    fully_connected1 = tf.nn.dropout(fully_connected1, keep_prob)\n",
    "    fully_connected2 = fully_conn(fully_connected1, 16)\n",
    "    fully_connected2 = tf.nn.dropout(fully_connected2, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    final_output = output(fully_connected1, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return final_output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    global valid_features, valid_labels\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "        \n",
    "    print ('Loss: {:>10.4f}   Validation Accuracy: {:0.6f}'.format(loss, valid_acc))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1810   Validation Accuracy: 0.167200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.1458   Validation Accuracy: 0.219200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.0647   Validation Accuracy: 0.273000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.0104   Validation Accuracy: 0.301600\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.9726   Validation Accuracy: 0.327600\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.9487   Validation Accuracy: 0.347000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.9228   Validation Accuracy: 0.351600\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.9162   Validation Accuracy: 0.368800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.8871   Validation Accuracy: 0.375400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.8301   Validation Accuracy: 0.389800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.8001   Validation Accuracy: 0.395200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.8225   Validation Accuracy: 0.398600\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.7647   Validation Accuracy: 0.412800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.7265   Validation Accuracy: 0.419400\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.6521   Validation Accuracy: 0.426600\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.6358   Validation Accuracy: 0.428800\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.6048   Validation Accuracy: 0.436600\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.5783   Validation Accuracy: 0.437800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.5575   Validation Accuracy: 0.444200\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.5522   Validation Accuracy: 0.446800\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.5144   Validation Accuracy: 0.454800\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.4781   Validation Accuracy: 0.456000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.4295   Validation Accuracy: 0.463600\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.4308   Validation Accuracy: 0.462200\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.4127   Validation Accuracy: 0.463000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.3654   Validation Accuracy: 0.468400\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.3703   Validation Accuracy: 0.470800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.3733   Validation Accuracy: 0.471800\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.3155   Validation Accuracy: 0.470200\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.3376   Validation Accuracy: 0.479000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.3027   Validation Accuracy: 0.477800\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.2928   Validation Accuracy: 0.478600\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.2509   Validation Accuracy: 0.484200\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.2570   Validation Accuracy: 0.480000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.2770   Validation Accuracy: 0.481400\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.2210   Validation Accuracy: 0.487600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.2232   Validation Accuracy: 0.485800\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.2121   Validation Accuracy: 0.486000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.2253   Validation Accuracy: 0.487000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.1737   Validation Accuracy: 0.488000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.2043   Validation Accuracy: 0.492200\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.1624   Validation Accuracy: 0.492400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.1807   Validation Accuracy: 0.492400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.1386   Validation Accuracy: 0.493800\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.1595   Validation Accuracy: 0.492800\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.1265   Validation Accuracy: 0.499600\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.1263   Validation Accuracy: 0.497000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.1476   Validation Accuracy: 0.492000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.0829   Validation Accuracy: 0.498800\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.1003   Validation Accuracy: 0.495800\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.0919   Validation Accuracy: 0.498400\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.0755   Validation Accuracy: 0.495200\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.0735   Validation Accuracy: 0.500600\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.0880   Validation Accuracy: 0.491800\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.0446   Validation Accuracy: 0.497000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.0394   Validation Accuracy: 0.500200\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.0623   Validation Accuracy: 0.497800\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.0329   Validation Accuracy: 0.499200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.0113   Validation Accuracy: 0.498800\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.0404   Validation Accuracy: 0.496000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.0103   Validation Accuracy: 0.503200\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.9925   Validation Accuracy: 0.506800\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.0111   Validation Accuracy: 0.500000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.0247   Validation Accuracy: 0.499800\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.9694   Validation Accuracy: 0.499600\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.9967   Validation Accuracy: 0.496000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.9880   Validation Accuracy: 0.498400\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.9528   Validation Accuracy: 0.506800\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.9540   Validation Accuracy: 0.507400\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.9588   Validation Accuracy: 0.498600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.9605   Validation Accuracy: 0.503400\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.9630   Validation Accuracy: 0.506200\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.9652   Validation Accuracy: 0.496600\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.9288   Validation Accuracy: 0.503000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.9270   Validation Accuracy: 0.508000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.9427   Validation Accuracy: 0.506200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.9393   Validation Accuracy: 0.509200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.9603   Validation Accuracy: 0.495000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.9385   Validation Accuracy: 0.499400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.8852   Validation Accuracy: 0.509400\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.8932   Validation Accuracy: 0.507600\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.8763   Validation Accuracy: 0.512200\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.8623   Validation Accuracy: 0.510200\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.8580   Validation Accuracy: 0.515800\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.8782   Validation Accuracy: 0.505000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.8788   Validation Accuracy: 0.508800\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.8688   Validation Accuracy: 0.511200\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.8387   Validation Accuracy: 0.510400\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.8718   Validation Accuracy: 0.505000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.8439   Validation Accuracy: 0.513000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.8360   Validation Accuracy: 0.514800\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.7944   Validation Accuracy: 0.518800\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.8156   Validation Accuracy: 0.510800\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.8299   Validation Accuracy: 0.510800\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.8155   Validation Accuracy: 0.516400\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.8110   Validation Accuracy: 0.514200\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.7997   Validation Accuracy: 0.507400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.7654   Validation Accuracy: 0.518400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.7759   Validation Accuracy: 0.516000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.7724   Validation Accuracy: 0.517200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2962   Validation Accuracy: 0.182400\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.1945   Validation Accuracy: 0.215600\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.1352   Validation Accuracy: 0.241000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.0867   Validation Accuracy: 0.243600\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.0093   Validation Accuracy: 0.271800\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0409   Validation Accuracy: 0.296400\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.9879   Validation Accuracy: 0.324200\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.7887   Validation Accuracy: 0.338000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.9253   Validation Accuracy: 0.330400\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.8508   Validation Accuracy: 0.356200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.8568   Validation Accuracy: 0.377200\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.8336   Validation Accuracy: 0.382000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.5499   Validation Accuracy: 0.381000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.6852   Validation Accuracy: 0.399800\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.7179   Validation Accuracy: 0.409400\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.7358   Validation Accuracy: 0.420800\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.6870   Validation Accuracy: 0.424000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.4329   Validation Accuracy: 0.413000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.5182   Validation Accuracy: 0.432200\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.5965   Validation Accuracy: 0.432000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.6444   Validation Accuracy: 0.441200\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.6009   Validation Accuracy: 0.457600\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.3141   Validation Accuracy: 0.428600\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.4470   Validation Accuracy: 0.452400\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.5386   Validation Accuracy: 0.458400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.6034   Validation Accuracy: 0.463800\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.5280   Validation Accuracy: 0.462600\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.2660   Validation Accuracy: 0.449800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.4235   Validation Accuracy: 0.466600\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.5242   Validation Accuracy: 0.468200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.5825   Validation Accuracy: 0.470400\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.4537   Validation Accuracy: 0.475800\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.2105   Validation Accuracy: 0.461400\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.3378   Validation Accuracy: 0.485800\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.4532   Validation Accuracy: 0.481000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.5842   Validation Accuracy: 0.488000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.4146   Validation Accuracy: 0.485400\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.1560   Validation Accuracy: 0.476400\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.3553   Validation Accuracy: 0.494200\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.4522   Validation Accuracy: 0.489000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.5470   Validation Accuracy: 0.495600\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.3851   Validation Accuracy: 0.499200\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.1382   Validation Accuracy: 0.476200\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.2851   Validation Accuracy: 0.502600\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.4171   Validation Accuracy: 0.492000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.5545   Validation Accuracy: 0.509800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.3793   Validation Accuracy: 0.506000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.0952   Validation Accuracy: 0.495400\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.2549   Validation Accuracy: 0.508000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.3831   Validation Accuracy: 0.513600\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.5603   Validation Accuracy: 0.522200\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.3388   Validation Accuracy: 0.516600\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.0774   Validation Accuracy: 0.497400\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.2503   Validation Accuracy: 0.524400\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.3434   Validation Accuracy: 0.521800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.5573   Validation Accuracy: 0.518400\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.3370   Validation Accuracy: 0.527000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.0501   Validation Accuracy: 0.507000\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.2157   Validation Accuracy: 0.528800\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.3229   Validation Accuracy: 0.524000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5343   Validation Accuracy: 0.527800\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.3100   Validation Accuracy: 0.536000\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.0508   Validation Accuracy: 0.511000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.1811   Validation Accuracy: 0.537800\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.3192   Validation Accuracy: 0.521400\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.4849   Validation Accuracy: 0.542200\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.3001   Validation Accuracy: 0.541200\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.0450   Validation Accuracy: 0.521200\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.1623   Validation Accuracy: 0.546600\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.2864   Validation Accuracy: 0.539000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.5039   Validation Accuracy: 0.545600\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.2818   Validation Accuracy: 0.547600\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.9982   Validation Accuracy: 0.535200\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.1451   Validation Accuracy: 0.546600\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.2819   Validation Accuracy: 0.527800\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.4376   Validation Accuracy: 0.552400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.2702   Validation Accuracy: 0.552200\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.9856   Validation Accuracy: 0.541400\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.1301   Validation Accuracy: 0.548000\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.2491   Validation Accuracy: 0.537000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.4113   Validation Accuracy: 0.557200\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.2577   Validation Accuracy: 0.560200\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.9974   Validation Accuracy: 0.547200\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.1133   Validation Accuracy: 0.557400\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.2698   Validation Accuracy: 0.535000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.3736   Validation Accuracy: 0.558200\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.2609   Validation Accuracy: 0.554000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.9857   Validation Accuracy: 0.560800\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.0734   Validation Accuracy: 0.556000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.2054   Validation Accuracy: 0.546200\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.3737   Validation Accuracy: 0.562400\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.2628   Validation Accuracy: 0.565200\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.9958   Validation Accuracy: 0.553000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.0645   Validation Accuracy: 0.563600\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.2033   Validation Accuracy: 0.549000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.3601   Validation Accuracy: 0.558800\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.2384   Validation Accuracy: 0.563400\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.9688   Validation Accuracy: 0.565000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.0416   Validation Accuracy: 0.571000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.2029   Validation Accuracy: 0.554200\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.3083   Validation Accuracy: 0.567200\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.2414   Validation Accuracy: 0.570800\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.9553   Validation Accuracy: 0.569000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.0089   Validation Accuracy: 0.568800\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.1978   Validation Accuracy: 0.544600\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.2727   Validation Accuracy: 0.569400\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.2279   Validation Accuracy: 0.568200\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.9470   Validation Accuracy: 0.571800\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.9861   Validation Accuracy: 0.579000\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.1435   Validation Accuracy: 0.563600\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.3169   Validation Accuracy: 0.565000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.2160   Validation Accuracy: 0.567200\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.9166   Validation Accuracy: 0.573600\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.9914   Validation Accuracy: 0.569800\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.1676   Validation Accuracy: 0.556600\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.2242   Validation Accuracy: 0.568600\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.1643   Validation Accuracy: 0.573000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.9161   Validation Accuracy: 0.576600\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.9651   Validation Accuracy: 0.577800\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.1315   Validation Accuracy: 0.564400\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.2842   Validation Accuracy: 0.573800\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.1744   Validation Accuracy: 0.572400\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.9216   Validation Accuracy: 0.580800\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.9633   Validation Accuracy: 0.586600\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.1155   Validation Accuracy: 0.574200\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.1927   Validation Accuracy: 0.575800\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     1.1680   Validation Accuracy: 0.579600\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.8815   Validation Accuracy: 0.581600\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.9463   Validation Accuracy: 0.584400\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.0666   Validation Accuracy: 0.582800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.1556   Validation Accuracy: 0.582800\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     1.1070   Validation Accuracy: 0.586800\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.8886   Validation Accuracy: 0.588800\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.9451   Validation Accuracy: 0.587800\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.0877   Validation Accuracy: 0.573200\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.1851   Validation Accuracy: 0.579000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     1.1112   Validation Accuracy: 0.572000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.8826   Validation Accuracy: 0.590400\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.9036   Validation Accuracy: 0.595400\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.0958   Validation Accuracy: 0.575200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.1681   Validation Accuracy: 0.579800\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     1.1142   Validation Accuracy: 0.580800\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.8715   Validation Accuracy: 0.593800\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.8869   Validation Accuracy: 0.595400\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.0859   Validation Accuracy: 0.587600\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.1333   Validation Accuracy: 0.583000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     1.0895   Validation Accuracy: 0.585800\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.8513   Validation Accuracy: 0.595600\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.8901   Validation Accuracy: 0.594600\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     1.0367   Validation Accuracy: 0.593800\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.0897   Validation Accuracy: 0.588600\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     1.1034   Validation Accuracy: 0.588000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.8461   Validation Accuracy: 0.601600\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.8508   Validation Accuracy: 0.592800\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     1.0325   Validation Accuracy: 0.591400\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.0913   Validation Accuracy: 0.584200\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     1.0491   Validation Accuracy: 0.592600\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.8433   Validation Accuracy: 0.596800\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.8358   Validation Accuracy: 0.594800\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     1.0342   Validation Accuracy: 0.589000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.1058   Validation Accuracy: 0.590800\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     1.0351   Validation Accuracy: 0.592400\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.8375   Validation Accuracy: 0.603200\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.8206   Validation Accuracy: 0.597800\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     1.0333   Validation Accuracy: 0.585400\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.0731   Validation Accuracy: 0.594400\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     1.0516   Validation Accuracy: 0.594600\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.8195   Validation Accuracy: 0.602000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.8343   Validation Accuracy: 0.605000\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.9988   Validation Accuracy: 0.594000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.0252   Validation Accuracy: 0.599400\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     1.0194   Validation Accuracy: 0.600200\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.8125   Validation Accuracy: 0.609400\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.8128   Validation Accuracy: 0.606800\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     1.0096   Validation Accuracy: 0.600000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.0322   Validation Accuracy: 0.594000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     1.0061   Validation Accuracy: 0.601200\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.7913   Validation Accuracy: 0.609200\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.8141   Validation Accuracy: 0.599800\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.9714   Validation Accuracy: 0.597600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.0474   Validation Accuracy: 0.595600\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     1.0257   Validation Accuracy: 0.597600\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.7676   Validation Accuracy: 0.606800\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.8056   Validation Accuracy: 0.607200\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.9832   Validation Accuracy: 0.602000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.0202   Validation Accuracy: 0.596600\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.9873   Validation Accuracy: 0.608200\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.7956   Validation Accuracy: 0.611000\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.7932   Validation Accuracy: 0.615200\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.9774   Validation Accuracy: 0.604000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.0037   Validation Accuracy: 0.603000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.9719   Validation Accuracy: 0.611000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.7858   Validation Accuracy: 0.615400\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.7607   Validation Accuracy: 0.608600\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.9857   Validation Accuracy: 0.609200\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.9891   Validation Accuracy: 0.606600\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.9093   Validation Accuracy: 0.613800\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.7760   Validation Accuracy: 0.616400\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.7497   Validation Accuracy: 0.609600\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.9541   Validation Accuracy: 0.607600\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.9529   Validation Accuracy: 0.614800\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.9544   Validation Accuracy: 0.610200\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.7586   Validation Accuracy: 0.612200\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.7496   Validation Accuracy: 0.614000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.9623   Validation Accuracy: 0.613800\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.9623   Validation Accuracy: 0.613200\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.9598   Validation Accuracy: 0.607600\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.7747   Validation Accuracy: 0.615800\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.7367   Validation Accuracy: 0.610200\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.9291   Validation Accuracy: 0.618400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.9620   Validation Accuracy: 0.607600\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.9375   Validation Accuracy: 0.614200\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.7640   Validation Accuracy: 0.622200\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.7249   Validation Accuracy: 0.615000\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.9452   Validation Accuracy: 0.614000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.9902   Validation Accuracy: 0.601600\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.8924   Validation Accuracy: 0.615200\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.7247   Validation Accuracy: 0.620200\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.7181   Validation Accuracy: 0.617800\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.9526   Validation Accuracy: 0.612400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.9758   Validation Accuracy: 0.615800\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.9120   Validation Accuracy: 0.621400\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.7270   Validation Accuracy: 0.624200\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.7194   Validation Accuracy: 0.620000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.9595   Validation Accuracy: 0.605600\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.9515   Validation Accuracy: 0.613800\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.9200   Validation Accuracy: 0.615000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.7185   Validation Accuracy: 0.623400\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.6872   Validation Accuracy: 0.622800\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.9057   Validation Accuracy: 0.622400\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.9425   Validation Accuracy: 0.623800\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.8863   Validation Accuracy: 0.618200\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.6984   Validation Accuracy: 0.623200\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.7047   Validation Accuracy: 0.621600\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.9133   Validation Accuracy: 0.623600\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.9306   Validation Accuracy: 0.616400\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.8532   Validation Accuracy: 0.621200\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.7237   Validation Accuracy: 0.626200\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.7048   Validation Accuracy: 0.627400\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.9043   Validation Accuracy: 0.616600\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.9486   Validation Accuracy: 0.610200\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.8783   Validation Accuracy: 0.626400\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.7014   Validation Accuracy: 0.627600\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.6723   Validation Accuracy: 0.622800\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.8982   Validation Accuracy: 0.622600\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.9283   Validation Accuracy: 0.625000\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.8618   Validation Accuracy: 0.624000\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.7006   Validation Accuracy: 0.624600\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.6755   Validation Accuracy: 0.626800\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.8952   Validation Accuracy: 0.623600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.9167   Validation Accuracy: 0.620800\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.8443   Validation Accuracy: 0.631000\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.6868   Validation Accuracy: 0.627800\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.6618   Validation Accuracy: 0.624400\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.8744   Validation Accuracy: 0.625800\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.9352   Validation Accuracy: 0.619400\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.9076   Validation Accuracy: 0.616000\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.6986   Validation Accuracy: 0.629600\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.6632   Validation Accuracy: 0.625800\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.8599   Validation Accuracy: 0.624600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.0019   Validation Accuracy: 0.603200\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.8746   Validation Accuracy: 0.624400\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.6784   Validation Accuracy: 0.629000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.6532   Validation Accuracy: 0.620800\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.8749   Validation Accuracy: 0.620200\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.8931   Validation Accuracy: 0.622200\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.8592   Validation Accuracy: 0.626800\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.6829   Validation Accuracy: 0.635200\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.6615   Validation Accuracy: 0.622000\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.8670   Validation Accuracy: 0.627600\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.9108   Validation Accuracy: 0.623200\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.8512   Validation Accuracy: 0.626400\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.6552   Validation Accuracy: 0.635000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.6458   Validation Accuracy: 0.630800\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.8458   Validation Accuracy: 0.633400\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.9152   Validation Accuracy: 0.634200\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.8363   Validation Accuracy: 0.629200\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.6795   Validation Accuracy: 0.628400\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.6251   Validation Accuracy: 0.636800\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.8301   Validation Accuracy: 0.629800\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.8655   Validation Accuracy: 0.632000\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.8554   Validation Accuracy: 0.627200\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.6432   Validation Accuracy: 0.630600\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.6258   Validation Accuracy: 0.629200\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.8149   Validation Accuracy: 0.636400\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.9440   Validation Accuracy: 0.619200\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.8055   Validation Accuracy: 0.631600\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.6345   Validation Accuracy: 0.631200\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.6462   Validation Accuracy: 0.628400\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.8548   Validation Accuracy: 0.626800\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.8831   Validation Accuracy: 0.620400\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.8277   Validation Accuracy: 0.634400\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.6461   Validation Accuracy: 0.628400\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.6403   Validation Accuracy: 0.632600\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.8008   Validation Accuracy: 0.628000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.8429   Validation Accuracy: 0.633200\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.8304   Validation Accuracy: 0.631000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.6507   Validation Accuracy: 0.636400\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.6205   Validation Accuracy: 0.636200\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.8173   Validation Accuracy: 0.631600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.8744   Validation Accuracy: 0.630200\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.8325   Validation Accuracy: 0.633200\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.6232   Validation Accuracy: 0.629400\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.6328   Validation Accuracy: 0.637600\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.7798   Validation Accuracy: 0.634200\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.8740   Validation Accuracy: 0.625800\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.8164   Validation Accuracy: 0.635400\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.6303   Validation Accuracy: 0.633000\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.6222   Validation Accuracy: 0.625600\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.7965   Validation Accuracy: 0.635000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.8295   Validation Accuracy: 0.631000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.8012   Validation Accuracy: 0.629800\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.6325   Validation Accuracy: 0.637600\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.6472   Validation Accuracy: 0.632600\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.7837   Validation Accuracy: 0.635800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.8464   Validation Accuracy: 0.627400\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.7829   Validation Accuracy: 0.640000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.6313   Validation Accuracy: 0.635000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.6592   Validation Accuracy: 0.628000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.7953   Validation Accuracy: 0.634000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.8714   Validation Accuracy: 0.629000\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.8104   Validation Accuracy: 0.629400\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.6209   Validation Accuracy: 0.638400\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.6129   Validation Accuracy: 0.629400\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.8013   Validation Accuracy: 0.634600\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.8950   Validation Accuracy: 0.625600\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.8297   Validation Accuracy: 0.628600\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.5948   Validation Accuracy: 0.637200\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.6228   Validation Accuracy: 0.627200\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.7783   Validation Accuracy: 0.638800\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.8831   Validation Accuracy: 0.631000\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.8159   Validation Accuracy: 0.634800\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.6106   Validation Accuracy: 0.637800\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.6167   Validation Accuracy: 0.629600\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.7795   Validation Accuracy: 0.637600\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.7938   Validation Accuracy: 0.636200\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.8462   Validation Accuracy: 0.631400\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.5956   Validation Accuracy: 0.642600\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.5882   Validation Accuracy: 0.640600\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.8037   Validation Accuracy: 0.634200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.8135   Validation Accuracy: 0.636600\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.7675   Validation Accuracy: 0.632400\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.5878   Validation Accuracy: 0.645800\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.5835   Validation Accuracy: 0.641800\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.7825   Validation Accuracy: 0.635000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.8372   Validation Accuracy: 0.635400\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.7691   Validation Accuracy: 0.638400\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.6025   Validation Accuracy: 0.636600\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.5877   Validation Accuracy: 0.635400\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.7632   Validation Accuracy: 0.639200\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.8031   Validation Accuracy: 0.637800\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.7769   Validation Accuracy: 0.625800\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.5730   Validation Accuracy: 0.637800\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.5894   Validation Accuracy: 0.637800\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.7647   Validation Accuracy: 0.636600\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.8304   Validation Accuracy: 0.634000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.7486   Validation Accuracy: 0.634200\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.5734   Validation Accuracy: 0.640800\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.5983   Validation Accuracy: 0.638600\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.7599   Validation Accuracy: 0.636600\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.8355   Validation Accuracy: 0.638400\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.7714   Validation Accuracy: 0.634800\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.5695   Validation Accuracy: 0.637000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.5811   Validation Accuracy: 0.631400\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.7715   Validation Accuracy: 0.641000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.7931   Validation Accuracy: 0.632800\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.7490   Validation Accuracy: 0.638800\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.5663   Validation Accuracy: 0.640400\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.5876   Validation Accuracy: 0.645600\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.7362   Validation Accuracy: 0.645200\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.7815   Validation Accuracy: 0.634400\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.7613   Validation Accuracy: 0.636600\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.6066   Validation Accuracy: 0.636800\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.5926   Validation Accuracy: 0.639000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.7293   Validation Accuracy: 0.634400\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.7982   Validation Accuracy: 0.637600\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.7143   Validation Accuracy: 0.634600\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.5833   Validation Accuracy: 0.640800\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.5861   Validation Accuracy: 0.633400\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.7178   Validation Accuracy: 0.643400\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.7957   Validation Accuracy: 0.639200\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.7596   Validation Accuracy: 0.636000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.5692   Validation Accuracy: 0.642600\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.5811   Validation Accuracy: 0.638200\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.7305   Validation Accuracy: 0.639000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.7823   Validation Accuracy: 0.632000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.7334   Validation Accuracy: 0.639800\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.5648   Validation Accuracy: 0.640800\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.5981   Validation Accuracy: 0.633400\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.7322   Validation Accuracy: 0.639600\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.7811   Validation Accuracy: 0.638600\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.7097   Validation Accuracy: 0.640400\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.5451   Validation Accuracy: 0.640600\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.5701   Validation Accuracy: 0.637000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.7150   Validation Accuracy: 0.640000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.7998   Validation Accuracy: 0.638000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.7302   Validation Accuracy: 0.635800\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.5526   Validation Accuracy: 0.641000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.5640   Validation Accuracy: 0.631200\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.7168   Validation Accuracy: 0.639800\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.7606   Validation Accuracy: 0.643200\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.7268   Validation Accuracy: 0.637000\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.5487   Validation Accuracy: 0.644400\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.5574   Validation Accuracy: 0.644000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.7281   Validation Accuracy: 0.636200\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.7942   Validation Accuracy: 0.640000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.7315   Validation Accuracy: 0.638000\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.5628   Validation Accuracy: 0.641800\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.5463   Validation Accuracy: 0.647200\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.7221   Validation Accuracy: 0.637000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.7909   Validation Accuracy: 0.633400\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.6917   Validation Accuracy: 0.645000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.5574   Validation Accuracy: 0.646200\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.5724   Validation Accuracy: 0.642600\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.7375   Validation Accuracy: 0.637000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.7715   Validation Accuracy: 0.638800\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.7069   Validation Accuracy: 0.639600\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.5208   Validation Accuracy: 0.646400\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.5608   Validation Accuracy: 0.640000\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.7247   Validation Accuracy: 0.641200\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.7524   Validation Accuracy: 0.644000\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.6986   Validation Accuracy: 0.640800\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.5316   Validation Accuracy: 0.646000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.5572   Validation Accuracy: 0.644400\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.7012   Validation Accuracy: 0.639600\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.7518   Validation Accuracy: 0.642000\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.7247   Validation Accuracy: 0.639400\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.5507   Validation Accuracy: 0.645800\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.5232   Validation Accuracy: 0.645200\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.7014   Validation Accuracy: 0.637800\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.7593   Validation Accuracy: 0.639000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.6513   Validation Accuracy: 0.641400\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.5236   Validation Accuracy: 0.644600\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.5451   Validation Accuracy: 0.639400\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.6982   Validation Accuracy: 0.638000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.7670   Validation Accuracy: 0.641600\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.6857   Validation Accuracy: 0.639600\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.5625   Validation Accuracy: 0.641000\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.5542   Validation Accuracy: 0.643600\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.7098   Validation Accuracy: 0.641000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.7426   Validation Accuracy: 0.638800\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.6882   Validation Accuracy: 0.629400\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.5221   Validation Accuracy: 0.639000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.5330   Validation Accuracy: 0.645200\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.6843   Validation Accuracy: 0.638400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.7421   Validation Accuracy: 0.639000\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.6666   Validation Accuracy: 0.639400\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.5662   Validation Accuracy: 0.636000\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.5263   Validation Accuracy: 0.645000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.6970   Validation Accuracy: 0.638200\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.7456   Validation Accuracy: 0.639200\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.6745   Validation Accuracy: 0.641600\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.5388   Validation Accuracy: 0.644600\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.5435   Validation Accuracy: 0.644400\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.6861   Validation Accuracy: 0.641800\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.7809   Validation Accuracy: 0.644000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.6784   Validation Accuracy: 0.638800\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.5104   Validation Accuracy: 0.644600\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.5264   Validation Accuracy: 0.645000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.6562   Validation Accuracy: 0.641000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.7456   Validation Accuracy: 0.642200\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.6307   Validation Accuracy: 0.643600\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.5581   Validation Accuracy: 0.635600\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.5077   Validation Accuracy: 0.644000\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.6636   Validation Accuracy: 0.639400\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.7279   Validation Accuracy: 0.639000\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.6606   Validation Accuracy: 0.635400\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.5125   Validation Accuracy: 0.645800\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.5725   Validation Accuracy: 0.638200\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.6625   Validation Accuracy: 0.642600\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.7199   Validation Accuracy: 0.641800\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.6656   Validation Accuracy: 0.637000\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.5132   Validation Accuracy: 0.644400\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.5211   Validation Accuracy: 0.643800\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.6622   Validation Accuracy: 0.635200\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.7424   Validation Accuracy: 0.641400\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.6451   Validation Accuracy: 0.640000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.5209   Validation Accuracy: 0.643800\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.5184   Validation Accuracy: 0.648800\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.6557   Validation Accuracy: 0.641800\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.7103   Validation Accuracy: 0.645600\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.6619   Validation Accuracy: 0.638800\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.5063   Validation Accuracy: 0.646000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.5403   Validation Accuracy: 0.641000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.6581   Validation Accuracy: 0.642000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.7543   Validation Accuracy: 0.645600\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.6308   Validation Accuracy: 0.641600\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.5238   Validation Accuracy: 0.642000\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.5190   Validation Accuracy: 0.646400\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.6608   Validation Accuracy: 0.639400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.6976   Validation Accuracy: 0.645400\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.6268   Validation Accuracy: 0.646200\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.5107   Validation Accuracy: 0.643400\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.5131   Validation Accuracy: 0.648000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.6604   Validation Accuracy: 0.641200\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.6991   Validation Accuracy: 0.642400\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.6239   Validation Accuracy: 0.645000\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.5315   Validation Accuracy: 0.645200\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.5364   Validation Accuracy: 0.646400\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.6735   Validation Accuracy: 0.636600\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6359770569620253\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92d4897f28>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
